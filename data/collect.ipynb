{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment Result Analysis\n",
    "\n",
    "Evaluated systems:\n",
    "\n",
    "- Nos\n",
    "- Bsl-Cocytus\n",
    "- Bsl-PQ\n",
    "- Bsl-Split\n",
    "- Bsl-Repl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from pathlib import Path\n",
    "from typing import *\n",
    "\n",
    "EVALUATED_SYSTEMS = [\n",
    "    'nos',\n",
    "    'cocytus',\n",
    "    'pq',\n",
    "    'split',\n",
    "    'repl'\n",
    "]\n",
    "\n",
    "EVALUATED_VALUE_SIZES = [\n",
    "    '64',\n",
    "    '256',\n",
    "    '1k',\n",
    "    '4k'\n",
    "]\n",
    "\n",
    "CLUSTER_SIZE = 32\n",
    "\n",
    "from enum import Enum\n",
    "\n",
    "class Dist(Enum):\n",
    "    '''\n",
    "    Key distribution.\n",
    "    '''\n",
    "    UNIFORM = 0\n",
    "    ZIPF = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_dir(exp: str) -> Tuple[float, float, pd.Series]:\n",
    "    '''\n",
    "    Collect csv-format performance dumps from a directory.\n",
    "\n",
    "    ### Args:\n",
    "        `exp`: Absolute path to the experiment data directory.\n",
    "\n",
    "    ### Returns:\n",
    "        (thpt_max, thpt_mean, latencies)    \n",
    "    '''\n",
    "    \n",
    "    throughputs = []\n",
    "    latencies = []\n",
    "\n",
    "    for f in glob.glob(f'{exp}/*.csv'):\n",
    "        df = pd.read_csv(f)\n",
    "        \n",
    "        # Drop the first and the last row\n",
    "        df.drop(df.head(1).index, inplace=True)\n",
    "        df.drop(df.tail(1).index, inplace=True)\n",
    "\n",
    "        # Get throughput data, pending merge\n",
    "        throughput = df['throughput'].sort_index(ascending=False).reset_index(drop=True)\n",
    "\n",
    "        # Get latency data, merge instantly\n",
    "        latency = df[['get_avg', 'get_p50', 'get_p99', 'get_p999', 'put_avg', 'put_p50', 'put_p99', 'put_p999']]\n",
    "        latency = latency.mean(axis=0)\n",
    "        \n",
    "        throughputs.append(throughput)\n",
    "        latencies.append(latency)\n",
    "    \n",
    "    if len(throughputs) == 0 or len(latencies) == 0:\n",
    "        raise Exception(\"No data found\")\n",
    "\n",
    "    # Merge throughputs into a table\n",
    "    throughput_table = pd.concat(throughputs, axis=1).sort_index(ascending=False).reset_index(drop=True).replace(np.nan, 0)\n",
    "    throughputs = throughput_table.sum(axis=1).astype(\"int\") / 1e6\n",
    "\n",
    "    # Merge latencies\n",
    "    latency_table = pd.concat(latencies, axis=1)\n",
    "    # print(exp, latency_table)\n",
    "    latencies = latency_table.mean(axis=1).astype(\"int\") / 1e3\n",
    "    return throughputs.max(), throughputs.mean(), latencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_system_dir(system: str, config: str) -> str:\n",
    "    if system == 'repl':\n",
    "        config = '42' if config == '62' else '63'\n",
    "    return f'{system}-{config}'\n",
    "\n",
    "\n",
    "def fix_glob_pattern(dir: str, op: str) -> str:\n",
    "    if op == 'put':\n",
    "        dir = dir.replace('split-lb', 'split')\n",
    "    return dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 6.2: Microbenchmark\n",
    "\n",
    "1. 100%-GET and 100%-PUT\n",
    "    * peak throughputs\n",
    "    * latency-throughput curves\n",
    "2. YCSB throughputs\n",
    "3. Sensitivity analysis\n",
    "4. Centralization test\n",
    "5. Dynamic constraint adapt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_100getput(config: str, dist: Dist = Dist.ZIPF, output_dir: str = 'res'):\n",
    "    '''\n",
    "    Section 6.2.1(a)\n",
    "    ----------------\n",
    "    \n",
    "    Collect performances of 100%-GET/100%-PUT microbenchmark.\n",
    "\n",
    "    ### Args:\n",
    "        `config`: Erasure coding configuration, e.g., `42`, `62`, and `63`.\n",
    "        `dist`:   Key distribution.\n",
    "    '''\n",
    "    \n",
    "    notebook_path = Path(os.getcwd())\n",
    "    data_dir = notebook_path.parent / 'data'\n",
    "    infix = '' if dist == Dist.UNIFORM else 'zipf-'\n",
    "\n",
    "    # Prevent unnecessary re-run\n",
    "    output_file = data_dir / output_dir / f'getput-{infix}{config}.xlsx'\n",
    "\n",
    "    # Common-case performance\n",
    "    commoncase_dir = data_dir / 'normal'\n",
    "\n",
    "    # Ensure the directory exists\n",
    "    present_systems = []\n",
    "    for system in EVALUATED_SYSTEMS:\n",
    "        system_dir = commoncase_dir / get_system_dir(system, config)\n",
    "        if system_dir.is_dir():\n",
    "            present_systems.append(system)\n",
    "        else:\n",
    "            print(f'Directory {system_dir} does not exist!')\n",
    "\n",
    "    results = {}    \n",
    "    for op in ['get', 'put']:\n",
    "        df_thpt = None\n",
    "        \n",
    "        for value_sz in EVALUATED_VALUE_SIZES:\n",
    "            row_thpt = {}\n",
    "\n",
    "            for system in present_systems:\n",
    "                system_dir = commoncase_dir / get_system_dir(system, config)\n",
    "\n",
    "                op_long = 'updateonly' if op == 'put' else 'readonly'\n",
    "                glob_pattern = fix_glob_pattern(f'{system_dir}/{op_long}-{infix}{value_sz}-*', op)\n",
    "                g = list(glob.glob(glob_pattern))\n",
    "\n",
    "                if len(g) == 0:\n",
    "                    print(glob_pattern)\n",
    "                    raise Exception(f'No data found for ({op}, {system}, {value_sz})')\n",
    "                \n",
    "                exp_dir = g[0]\n",
    "                thpt_max, _, _ = collect_dir(exp_dir)\n",
    "                \n",
    "                row_thpt[system] = thpt_max\n",
    "            \n",
    "            df_thpt = pd.concat([df_thpt, pd.DataFrame(row_thpt, index=[value_sz])])\n",
    "        \n",
    "        results[op] = df_thpt\n",
    "    \n",
    "    # Dump\n",
    "    with pd.ExcelWriter(output_file, engine='openpyxl') as writer:\n",
    "        for op in ['get', 'put']:\n",
    "            df_thpt = results[op]\n",
    "            df_thpt.to_excel(writer, sheet_name=f'{op}-thpt')\n",
    "\n",
    "\n",
    "# Do collection\n",
    "collect_100getput('42', Dist.UNIFORM)\n",
    "collect_100getput('62', Dist.UNIFORM)\n",
    "collect_100getput('63', Dist.UNIFORM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_ltc(output_dir: str = 'res'):\n",
    "    '''\n",
    "    Section 6.2.1(b)\n",
    "    ----------------\n",
    "\n",
    "    Collect latency-throughput curve data.\n",
    "    '''\n",
    "\n",
    "    notebook_path = Path(os.getcwd())\n",
    "    data_dir = notebook_path.parent / 'data'\n",
    "    ltc_dir = data_dir / 'ltc'\n",
    "\n",
    "    def do_collect_op(systems: List[str], op: str):\n",
    "        output_file = data_dir / output_dir / f'ltc-{op}.xlsx'\n",
    "        results = {}\n",
    "        op_long = 'readonly' if op == 'get' else 'updateonly'\n",
    "\n",
    "        for system in systems:\n",
    "            df_res = None\n",
    "            \n",
    "            glob_pattern = f'{ltc_dir}/{system}/{op_long}-*.csv'\n",
    "            for f in glob.glob(glob_pattern):\n",
    "                offered_load = int(f.split('-')[-1].split('.')[0])\n",
    "\n",
    "                df = pd.read_csv(f)\n",
    "        \n",
    "                # Drop the first and the last row\n",
    "                df.drop(df.head(1).index, inplace=True)\n",
    "                df.drop(df.tail(1).index, inplace=True)\n",
    "\n",
    "                # Get throughput data, pending merge\n",
    "                throughput = df['throughput'].sort_index(ascending=False).reset_index(drop=True)\n",
    "                throughput = throughput.max()\n",
    "\n",
    "                # Get latency data, merge instantly\n",
    "                latency = df[[f'{op}_avg', f'{op}_p50', f'{op}_p99']]\n",
    "                latency = latency.mean(axis=0)\n",
    "\n",
    "                row = { 'thpt': offered_load + throughput / 1e6 }\n",
    "                row[f'{op}_avg'] = latency[f'{op}_avg'] / 1e3\n",
    "                row[f'{op}_p50'] = latency[f'{op}_p50'] / 1e3\n",
    "                row[f'{op}_p99'] = latency[f'{op}_p99'] / 1e3\n",
    "            \n",
    "                df_res = pd.concat([df_res, pd.DataFrame(row, index=[offered_load])])\n",
    "            \n",
    "            # Sort by index\n",
    "            df_res.sort_index(inplace=True)\n",
    "            \n",
    "            results[system] = df_res\n",
    "\n",
    "        with pd.ExcelWriter(output_file, engine='openpyxl') as writer:\n",
    "            for system in systems:\n",
    "                df = results[system]\n",
    "                df.to_excel(writer, sheet_name=system)\n",
    "\n",
    "    # Read latency curve.\n",
    "    # 3 types: Nos, Split, Split-LB.\n",
    "    do_collect_op(['nos', 'split', 'split-lb'], 'get')\n",
    "\n",
    "    # Write latency curve.\n",
    "    # 5 typs:  Nos, Split, Cocytus, PQ, Repl.\n",
    "    do_collect_op(['nos', 'split', 'cocytus', 'pq', 'repl'], 'put')\n",
    "\n",
    "\n",
    "collect_ltc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_ycsb(config: str, output_dir: str = 'res'):\n",
    "    '''\n",
    "    Section 6.2.2\n",
    "    -------------\n",
    "\n",
    "    Collect performances of YCSB benchmark.\n",
    "\n",
    "    ### Args:\n",
    "        `config`: Erasure coding configuration, e.g., `42`, `62`, and `63`.\n",
    "    '''\n",
    "    \n",
    "    notebook_path = Path(os.getcwd())\n",
    "    data_dir = notebook_path.parent / 'data'\n",
    "\n",
    "    # Prevent unnecessary re-run.\n",
    "    output_file = data_dir / output_dir / f'ycsb-{config}.xlsx'\n",
    "    if output_file.is_file():\n",
    "        return\n",
    "\n",
    "    # Ensure the directory exists.\n",
    "    present_systems = []\n",
    "    for system in EVALUATED_SYSTEMS:\n",
    "        system_dir = data_dir / 'ycsb' / get_system_dir(system, config)\n",
    "        if system_dir.is_dir():\n",
    "            present_systems.append(system)\n",
    "        else:\n",
    "            print(f'Directory {system_dir} does not exist!')\n",
    "\n",
    "    df_thpt = None\n",
    "    for workload in ['ycsb-a', 'ycsb-b', 'ycsb-d']:\n",
    "        row_thpt = {}\n",
    "        for system in present_systems:\n",
    "            system_dir = data_dir / 'ycsb' / get_system_dir(system, config)\n",
    "\n",
    "            glob_pattern = f'{system_dir}/{workload}-*'\n",
    "            g = list(glob.glob(glob_pattern))\n",
    "\n",
    "            if len(g) == 0:\n",
    "                raise Exception(f'No data found for ({workload}, {system})')\n",
    "            \n",
    "            exp_dir = g[0]\n",
    "            thpt_max, _, _ = collect_dir(exp_dir)\n",
    "            \n",
    "            row_thpt[system] = thpt_max\n",
    "            \n",
    "        df_thpt = pd.concat([df_thpt, pd.DataFrame(row_thpt, index=[workload])])\n",
    "    \n",
    "    # Dump\n",
    "    with pd.ExcelWriter(output_file, engine='openpyxl') as writer:\n",
    "        df_thpt.to_excel(writer, sheet_name='thpt')\n",
    "\n",
    "\n",
    "# Do collection\n",
    "collect_ycsb('42')\n",
    "collect_ycsb('62')\n",
    "collect_ycsb('63')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_sensitivity(output_dir: str = 'res'):\n",
    "    '''\n",
    "    Section 6.2.3\n",
    "    -------------\n",
    "\n",
    "    Collect performances of EC parameter sensitivity tests.\n",
    "    '''\n",
    "\n",
    "    notebook_path = Path(os.getcwd())\n",
    "    data_dir = notebook_path.parent / 'data'\n",
    "\n",
    "    output_file = data_dir / output_dir / f'sensitivity.xlsx'\n",
    "    sensitivity_dir = data_dir / 'sensitivity'\n",
    "\n",
    "    present_systems = list(x for x in EVALUATED_SYSTEMS if x != 'pq')\n",
    "\n",
    "    # Enumerate configurations\n",
    "    df = { 4: None, 6: None }\n",
    "    for k in [4, 6]:\n",
    "        df_thpt = None\n",
    "        for p in range(1, k):\n",
    "            row_thpt = {}\n",
    "            for system in present_systems:\n",
    "                exp_dir = sensitivity_dir / system / f'{k}{p}'\n",
    "                thpt_max, _, _ = collect_dir(exp_dir)\n",
    "\n",
    "                row_thpt[system] = thpt_max\n",
    "        \n",
    "            df_thpt = pd.concat([df_thpt, pd.DataFrame(row_thpt, index=[p])])\n",
    "\n",
    "        df[k] = df_thpt\n",
    "\n",
    "    # Dump\n",
    "    with pd.ExcelWriter(output_file, engine='openpyxl') as writer:\n",
    "        df[4].to_excel(writer, sheet_name='k4')\n",
    "        df[6].to_excel(writer, sheet_name='k6')\n",
    "    \n",
    "\n",
    "# Do collection\n",
    "collect_sensitivity()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_centralized(output_dir: str = 'res'):\n",
    "    '''\n",
    "    Section 6.2.4\n",
    "    -------------\n",
    "\n",
    "    Collect performances with centralized MDS.\n",
    "    '''\n",
    "\n",
    "    notebook_path = Path(os.getcwd())\n",
    "    data_dir = notebook_path.parent / 'data'\n",
    "\n",
    "    output_file = data_dir / output_dir / f'centralized.xlsx'\n",
    "    centralized_dir = data_dir / 'centralized'\n",
    "\n",
    "    present_systems = ['mds'] + EVALUATED_SYSTEMS\n",
    "\n",
    "    # Enumerate configurations\n",
    "    df_thpt = None\n",
    "    for exp in ['read', 'update']:\n",
    "        row_thpt = {}\n",
    "        for system in present_systems:\n",
    "            system_dir = centralized_dir / system\n",
    "\n",
    "            glob_pattern = f'{system_dir}/{exp}only-*'\n",
    "            g = list(glob.glob(glob_pattern))\n",
    "\n",
    "            if len(g) == 0:\n",
    "                print(glob_pattern)\n",
    "                raise Exception(f'No data found for ({exp}, {system})')\n",
    "            \n",
    "            exp_dir = g[0]\n",
    "            thpt_max, _, _ = collect_dir(exp_dir)\n",
    "            row_thpt[system] = thpt_max\n",
    "    \n",
    "        df_thpt = pd.concat([df_thpt, pd.DataFrame(row_thpt, index=[exp])])\n",
    "\n",
    "    # Dump\n",
    "    with pd.ExcelWriter(output_file, engine='openpyxl') as writer:\n",
    "        df_thpt.to_excel(writer, sheet_name='thpt')\n",
    "    \n",
    "\n",
    "# Do collection\n",
    "collect_centralized()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_slowdown(output_dir: str = 'res'):\n",
    "    '''\n",
    "    Section 6.2.5\n",
    "    -------------\n",
    "\n",
    "    Collect performances with dynamic slowdown.\n",
    "    '''\n",
    "\n",
    "    notebook_path = Path(os.getcwd())\n",
    "    data_dir = notebook_path.parent / 'data'\n",
    "\n",
    "    output_file = data_dir / output_dir / 'slowdown.xlsx'\n",
    "    slowdown_dir = data_dir / 'slowdown'\n",
    "\n",
    "    present_systems = ['cocytus', 'nos', 'repl', 'split', 'repl+', 'split+', 'dynbackup', 'repl-handoff']\n",
    "\n",
    "    results = {}\n",
    "\n",
    "    # Collect curve data, one sheet per system\n",
    "    for sys in present_systems:\n",
    "        system_dir = f'{slowdown_dir}/ycsb-d-{sys}'\n",
    "        \n",
    "        reclen = 999999\n",
    "        for f in glob.glob(f'{system_dir}/*.csv'):\n",
    "            df = pd.read_csv(f)\n",
    "            reclen = min(reclen, df.shape[0])\n",
    "\n",
    "        thpt = None\n",
    "        lats = None\n",
    "\n",
    "        for f in glob.glob(f'{system_dir}/*.csv'):\n",
    "            df = pd.read_csv(f)\n",
    "            df = df.truncate(after=reclen)\n",
    "\n",
    "            if thpt is None:\n",
    "                thpt = df['throughput']\n",
    "            else:\n",
    "                thpt = thpt.add(df['throughput'], fill_value=0)\n",
    "            \n",
    "            latency = df[['get_p50', 'get_p99', 'put_p50', 'put_p99']]\n",
    "            lats = pd.concat([lats, latency])\n",
    "        \n",
    "        lats = lats.groupby(lats.index).mean().astype(int)\n",
    "        total = pd.concat([thpt / 1e6, lats / 1e3], axis=1)\n",
    "        results[sys] = total\n",
    "\n",
    "    # Dump\n",
    "    with pd.ExcelWriter(output_file, engine='openpyxl') as writer:\n",
    "        for sys in results:\n",
    "            results[sys].to_excel(writer, sheet_name=sys)\n",
    "\n",
    "\n",
    "# Do collection\n",
    "collect_slowdown()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 6.3: Twemcache\n",
    "\n",
    "5. GET/PUT latency-throughput curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_twemcache(cluster: str, output_dir: str = 'res'):\n",
    "    '''\n",
    "    Section 6.3\n",
    "    -----------\n",
    "\n",
    "    Collect Twemcache trace throughput-latency curves.\n",
    "    '''\n",
    "\n",
    "    notebook_path = Path(os.getcwd())\n",
    "    data_dir = notebook_path.parent / 'data'\n",
    "    twemcache_dir = data_dir / 'twemcache' / cluster\n",
    "    \n",
    "    results = {}\n",
    "    for system in EVALUATED_SYSTEMS:\n",
    "        system_dir = twemcache_dir / system\n",
    "        df_res = None\n",
    "        \n",
    "        # Closed-loop throughput (regarded as the maximum)\n",
    "        closed_loop_dir = f'{system_dir}/closed'\n",
    "        closed_thpt, _, closed_lat = collect_dir(closed_loop_dir)\n",
    "\n",
    "        # Open-loop throughputs\n",
    "        glob_pattern = f'{system_dir}/*.csv'\n",
    "        for f in glob.glob(glob_pattern):\n",
    "            offered_load = int(f.split('/')[-1].split('.')[0])\n",
    "            if offered_load > closed_thpt:\n",
    "                closed_thpt = 0\n",
    "\n",
    "            df = pd.read_csv(f)\n",
    "\n",
    "            # Drop the first and the last row\n",
    "            df.drop(df.head(1).index, inplace=True)\n",
    "            df.drop(df.tail(1).index, inplace=True)\n",
    "\n",
    "            # Get throughput data, pending merge\n",
    "            throughput = df['throughput'].sort_index(ascending=False).reset_index(drop=True)\n",
    "            throughput = throughput.max()\n",
    "\n",
    "            # Get latency data, merge instantly\n",
    "            latency = df[['get_p50', 'get_p99', 'put_p50', 'put_p99']]\n",
    "            latency = latency.mean(axis=0)\n",
    "\n",
    "            row = { 'thpt': offered_load + throughput / 1e6 }\n",
    "            row['get_p50'] = latency['get_p50'] / 1e3\n",
    "            row['get_p99'] = latency['get_p99'] / 1e3\n",
    "            row['put_p50'] = latency['put_p50'] / 1e3\n",
    "            row['put_p99'] = latency['put_p99'] / 1e3\n",
    "        \n",
    "            df_res = pd.concat([df_res, pd.DataFrame(row, index=[offered_load])])\n",
    "            \n",
    "        if closed_thpt > 0:\n",
    "            row = { 'thpt': closed_thpt }\n",
    "            row['get_p50'] = closed_lat['get_p50']\n",
    "            row['get_p99'] = closed_lat['get_p99']\n",
    "            row['put_p50'] = closed_lat['put_p50']\n",
    "            row['put_p99'] = closed_lat['put_p99']\n",
    "            df_res = pd.concat([df_res, pd.DataFrame(row, index=[closed_thpt])])\n",
    "\n",
    "        # Sort by index\n",
    "        df_res.sort_index(inplace=True)\n",
    "        results[system] = df_res\n",
    "\n",
    "\n",
    "    output_file = data_dir / output_dir / f'twemcache-{cluster}.xlsx'\n",
    "    with pd.ExcelWriter(output_file, engine='openpyxl') as writer:\n",
    "        for system in EVALUATED_SYSTEMS:\n",
    "            df_thpt = results[system]\n",
    "            df_thpt.to_excel(writer, sheet_name=system)\n",
    "\n",
    "\n",
    "CLUSTERS = ['04', '12', '27', '31']\n",
    "for cluster in CLUSTERS:\n",
    "    collect_twemcache(cluster)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 6.4: Recovery\n",
    "\n",
    "6. Recovery throughputs\n",
    "7. Degraded read latencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_recovery(output_dir: str = 'res'):\n",
    "    '''\n",
    "    Section 6.4(a)\n",
    "    --------------\n",
    "\n",
    "    Collect recovery throughput.\n",
    "    '''\n",
    "    \n",
    "    notebook_path = Path(os.getcwd())\n",
    "    data_dir = notebook_path.parent / 'data'\n",
    "\n",
    "    # Prevent unnecessary re-run\n",
    "    output_file = data_dir / output_dir / f'recovery.xlsx'\n",
    "    if output_file.is_file():\n",
    "        return\n",
    "\n",
    "    # Recovery performance\n",
    "    recovery_dir = data_dir / 'recovery'\n",
    "\n",
    "    # Ensure the directory exists\n",
    "    for system in EVALUATED_SYSTEMS:\n",
    "        system_dir = recovery_dir / system\n",
    "        if not system_dir.is_dir():\n",
    "            raise Exception(f'Directory {system_dir} does not exist')\n",
    "\n",
    "    AMPLIFICATION = {\n",
    "        'nos': 1.75,\n",
    "        'cocytus': 1.5,\n",
    "        'pq': 1.5,\n",
    "        'repl': 3,\n",
    "        'split': 1.5,\n",
    "    }\n",
    "    TOTAL_OBJECTS = 50e6 / 32\n",
    "\n",
    "    df_res = None\n",
    "    for value_sz in EVALUATED_VALUE_SIZES:\n",
    "        row = {}\n",
    "        for system in EVALUATED_SYSTEMS:\n",
    "            system_dir = recovery_dir / system\n",
    "\n",
    "            df = pd.read_csv(f'{system_dir}/{value_sz}.csv')\n",
    "        \n",
    "            # Drop the first and the last row\n",
    "            df.drop(df.head(1).index, inplace=True)\n",
    "            df.drop(df.tail(1).index, inplace=True)\n",
    "\n",
    "            # Get throughput data, pending merge\n",
    "            throughput = df['throughput'].sort_index(ascending=False).reset_index(drop=True)\n",
    "            throughput = throughput.max()\n",
    "\n",
    "            row[system] = TOTAL_OBJECTS / throughput * AMPLIFICATION[system]\n",
    "        \n",
    "        df_res = pd.concat([df_res, pd.DataFrame(row, index=[value_sz])])\n",
    "\n",
    "    # Dump\n",
    "    with pd.ExcelWriter(output_file, engine='openpyxl') as writer:\n",
    "        df_res.to_excel(writer, sheet_name='recovery')\n",
    "\n",
    "\n",
    "# Do collection\n",
    "collect_recovery()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_degraded():\n",
    "    '''\n",
    "    Section 6.4(b)\n",
    "    --------------\n",
    "\n",
    "    Degraded read latency collector placeholder.\n",
    "    '''\n",
    "\n",
    "    raise NotImplementedError(\"degraded read latency is hand-collected!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 6.5: Memory Consumption\n",
    "\n",
    "8. Memory consumption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_memory(output_dir: str = 'res'):\n",
    "    notebook_path = Path(os.getcwd())\n",
    "    data_dir = notebook_path.parent / 'data'\n",
    "\n",
    "    data_file = data_dir / 'memory.txt'\n",
    "    output_file = data_dir / output_dir / 'memory.xlsx'\n",
    "\n",
    "    present_systems = list(x for x in EVALUATED_SYSTEMS if x != 'pq')\n",
    "    results = {}\n",
    "    for line in open(data_file, 'r').readlines():\n",
    "        parts = line.strip().split()\n",
    "        sys = parts[0]\n",
    "        conf = parts[1]\n",
    "        value_size = parts[2]\n",
    "        mem = int(parts[3])\n",
    "\n",
    "        if sys.startswith('bsl-'):\n",
    "            sys = sys[4:]\n",
    "\n",
    "        if conf not in results:\n",
    "            results[conf] = pd.DataFrame(columns=present_systems, index=EVALUATED_VALUE_SIZES)\n",
    "        \n",
    "        actual_mem_gb = mem * CLUSTER_SIZE / 1e6\n",
    "        results[conf].at[value_size, sys] = actual_mem_gb\n",
    "\n",
    "    with pd.ExcelWriter(output_file, engine='openpyxl') as writer:\n",
    "        for config in results:\n",
    "            df = results[config]\n",
    "            df.to_excel(writer, sheet_name=config)\n",
    "\n",
    "\n",
    "collect_memory()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
